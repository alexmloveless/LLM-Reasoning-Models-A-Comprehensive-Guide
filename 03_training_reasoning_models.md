# Chapter 3: Training LLM Reasoning Models

Training reasoning models represents one of the most significant advances in machine learning methodology. Unlike traditional language models that learn primarily from predicting the next word in ordinary text, reasoning models require a fundamentally different approach—one that teaches them not just to generate plausible-sounding text, but to solve problems through systematic thinking. The training process is where the sophistication emerges, transforming a standard transformer architecture into a system capable of systematic reasoning.

The journey begins with data, but not just any data. Training reasoning models requires carefully curated datasets that pair problems with detailed reasoning traces. These aren't simply question-answer pairs; they're complete worked examples that show every step of the problem-solving process. Imagine a textbook where every problem includes not just the final answer but a detailed explanation of how to think through the problem, what approaches to consider, why certain methods work or don't work, and how to verify the solution. That's essentially what reasoning model training data looks like.

Creating this training data is itself a monumental task. Some of it comes from educational materials—textbooks, worked examples, tutorial websites. But much of it needs to be generated specifically for training purposes. This often involves human experts working through problems while carefully documenting their thought processes. For mathematical problems, this might mean showing every algebraic manipulation, explaining why each step is taken, and noting alternative approaches. For coding problems, it means not just providing the solution but explaining the algorithm design, discussing trade-offs, and showing how to test and debug the code.

But human-generated data, whilst high quality, is expensive and limited in scale. This is where synthetic data generation becomes crucial. Researchers have developed sophisticated techniques for automatically generating reasoning traces. One approach involves using existing reasoning models to solve problems and generate explanations, then having humans or other models verify and refine these traces. Another technique uses formal systems—such as computer algebra systems or automated theorem provers—to generate mathematically rigorous solution paths that can then be translated into natural language explanations.

The actual training process for reasoning models involves several stages, each designed to build different capabilities. The first stage often involves pre-training on a large corpus of general text, similar to traditional language models. This gives the model a broad foundation of knowledge and basic language understanding. But unlike traditional LLMs, this is just the beginning.

The second stage introduces reasoning-specific training. Here, the model learns to generate reasoning traces when presented with problems. This isn't as simple as just training on problem-solution pairs. The model needs to learn several interconnected skills: recognising when extended reasoning is needed, breaking down problems into steps, maintaining consistency across long reasoning chains, and formatting solutions clearly.

One of the key innovations in training reasoning models is the use of reinforcement learning techniques, particularly Reinforcement Learning from Human Feedback (RLHF) and its variants. After initial supervised training on reasoning traces, models undergo additional training where they attempt to solve problems on their own. Their solutions are evaluated not just on whether they reach the correct answer, but on the quality of their reasoning process. This evaluation might be done by humans, by automated verification systems, or by other models trained specifically for this purpose—often called process reward models.

The reward structure in this reinforcement learning phase is carefully designed. Simply rewarding correct answers would lead to models that guess or use shortcuts rather than reasoning properly. Instead, the reward function considers multiple factors: Is the reasoning logically sound? Are all steps justified? Is the explanation clear and complete? Does the model appropriately express uncertainty when warranted? This complex reward structure encourages models to develop genuine reasoning capabilities rather than just pattern matching.

Another crucial aspect of training is teaching models to recognise and recover from errors. This involves training on examples that include mistakes and corrections. The model sees instances where an initial approach fails, followed by recognition of the error and a corrected approach. This teaches the model that it's acceptable to make mistakes as long as they're recognised and fixed—a crucial aspect of real problem-solving.

The computational requirements for training reasoning models are staggering. Not only do these models tend to be large—often hundreds of billions of parameters—but the training process itself is more complex than standard language model training. Each training example might involve thousands of tokens of reasoning, and the reinforcement learning phase requires generating and evaluating millions of solution attempts. This has led to innovations in distributed training techniques and more efficient training algorithms.

One particularly interesting aspect of training is handling the diversity of reasoning styles. Different experts might approach the same problem in different ways, all equally valid. The training process needs to accommodate this diversity whilst still maintaining consistency. This is achieved through techniques such as mixture-of-experts training, where the model learns to recognise and employ different reasoning strategies as appropriate.

The role of curriculum learning in training reasoning models cannot be overstated. Just as human education progresses from simple to complex topics, reasoning models benefit from a carefully structured training curriculum. Early in training, the model might work on simple arithmetic or basic logical puzzles. As it develops competence, the problems become more complex, requiring longer reasoning chains and more sophisticated techniques. This gradual progression helps the model build a solid foundation before tackling advanced problems.

Quality control during training is paramount. Unlike traditional language models where some errors might go unnoticed in the massive scale of training data, reasoning traces need to be accurate. A single error in a mathematical derivation or a flawed logical step can corrupt the model's learning. This has led to sophisticated data validation pipelines that use a combination of automated checking, cross-validation with formal systems, and human review.

The evaluation of reasoning models during training presents unique challenges. Traditional metrics like perplexity, while useful, don't capture reasoning quality. New metrics have been developed that evaluate not just correctness but reasoning depth, clarity, and robustness. These might involve testing the model on variations of problems it has seen, checking if it can apply learned techniques to novel situations, and assessing whether it can explain its reasoning in different ways.

One of the most fascinating discoveries in training reasoning models is the emergence of "reasoning shortcuts" and how to prevent them. Early training attempts sometimes resulted in models that appeared to reason but were actually using sophisticated pattern matching. For instance, a model might learn that certain problem phrasings typically lead to certain answer patterns without actually understanding the underlying logic. Preventing this requires careful dataset design, adversarial training examples, and evaluation protocols that test true understanding.

The interplay between different types of reasoning during training is another crucial consideration. Mathematical reasoning, logical reasoning, causal reasoning, and common-sense reasoning all have different characteristics, yet a comprehensive reasoning model needs to integrate all of these. The training process must balance exposure to different reasoning types whilst helping the model recognise which type is appropriate for a given problem.

Transfer learning plays a vital role in making reasoning model training practical. Rather than training each model from scratch, researchers have developed techniques for fine-tuning existing models to add reasoning capabilities. This might involve starting with a strong language model and adding reasoning-specific layers or training protocols. This approach significantly reduces the computational requirements while still achieving strong reasoning performance.

The iterative nature of reasoning model training reflects the complexity of the task. Unlike traditional models that might converge after a certain amount of training, reasoning models often benefit from multiple rounds of training with different focuses. One round might emphasise accuracy, another might focus on explanation clarity, and yet another might work on handling edge cases and unusual problems.

As training progresses, interesting phenomena emerge. Models develop their own "style" of reasoning, sometimes discovering problem-solving techniques that weren't explicitly in the training data. They learn to recognise patterns across different domains and to apply techniques learned in one area to problems in another. This kind of generalisation is one of the most promising aspects of reasoning models.

The future of reasoning model training holds even more possibilities. Researchers are exploring techniques such as constitutional training, where models learn not just from examples but from high-level principles about good reasoning. Others are investigating how to incorporate formal verification systems directly into the training loop, ensuring that mathematical and logical reasoning is not just plausible but provably correct.

The training of reasoning models represents a new paradigm in machine learning—one that goes beyond pattern recognition to genuine problem-solving. It's a complex, computationally intensive process that requires innovations in data generation, training algorithms, and evaluation metrics. But the results—models that can engage in sustained, logical reasoning—justify the effort. As we'll see in the next chapter, understanding how these models actually reason opens up even more fascinating questions about the nature of machine intelligence.
