# Chapter 2: How LLM Reasoning Models Work

Understanding how reasoning models work requires peeling back several layers of complexity. At their core, these models build upon the transformer architecture that revolutionized natural language processing, but they incorporate crucial modifications and training approaches that enable them to engage in structured, multi-step reasoning. The magic lies not in any single innovation, but in the careful orchestration of multiple techniques that work together to create systems capable of genuine problem-solving.

The foundation of reasoning models remains the transformer architecture, with its self-attention mechanisms that allow the model to consider relationships between all parts of an input simultaneously. However, reasoning models diverge from traditional LLMs in how they process and generate information. Where a traditional LLM might generate text in a single forward pass, reasoning models are designed to engage in what we might call "computational thinking time."

This thinking time is perhaps the most distinctive feature of reasoning models. When presented with a complex problem, these models don't immediately jump to an answer. Instead, they engage in an internal process of reasoning that can involve thousands or even tens of thousands of tokens of "thought." This internal monologue isn't just for show—it represents the model genuinely working through the problem, considering different approaches, checking its logic, and building toward a solution.

The mechanism that enables this extended reasoning process is sophisticated. During training, reasoning models learn to generate what researchers call "reasoning traces"—detailed step-by-step explanations of how to solve problems. These traces serve multiple purposes. First, they force the model to break down complex problems into manageable steps. Second, they create a kind of "working memory" where the model can keep track of intermediate results and logical connections. Third, they provide a form of error checking, as the model can review its own reasoning and catch mistakes.

But how does a model learn to reason rather than merely imitate the surface features of reasoning? The answer lies in the training process, which we'll explore in detail in the next chapter. For now, it's important to understand that reasoning models are trained on vast datasets of problems paired with detailed solutions. Crucially, these aren't just correct answers—they're complete reasoning traces that show every step of the problem-solving process.

The architecture of reasoning models also includes specialized components designed to support extended reasoning. One key innovation is the use of "reasoning tokens"—special markers that signal to the model when it should engage in step-by-step thinking versus when it should provide a direct response. These tokens act like cognitive switches, allowing the model to shift between different modes of operation as needed.

Another crucial component is the attention mechanism's modification to handle long reasoning chains. Traditional transformers can struggle with very long sequences, as the computational cost of attention grows quadratically with sequence length. Reasoning models employ various techniques to manage this challenge, including sparse attention patterns that focus on the most relevant parts of the reasoning trace and hierarchical processing that allows the model to work with abstractions rather than getting bogged down in details.

The generation process in reasoning models is also fundamentally different from traditional LLMs. Rather than generating text token by token in a purely forward manner, reasoning models engage in a more complex process that might involve backtracking, revision, and exploration of multiple solution paths. This is achieved through techniques like beam search with special scoring functions that evaluate not just the likelihood of the next token, but the logical coherence of the entire reasoning chain.

One fascinating aspect of how reasoning models work is their ability to maintain consistency across long chains of reasoning. This is achieved through what researchers call "state tracking"—the model's ability to keep track of what it has established, what assumptions it's working under, and what remains to be proven or explored. This state tracking isn't explicit in the way a traditional program might maintain variables; instead, it emerges from the model's learned representations and the structure of its attention patterns.

The model's approach to problem decomposition is another key element of how reasoning works. When faced with a complex problem, reasoning models have learned to automatically break it down into subproblems. This decomposition isn't hardcoded—it emerges from the training process. The model learns to recognize patterns in problem structure and to apply appropriate decomposition strategies. For a mathematical proof, this might mean identifying lemmas that need to be proven first. For a coding problem, it might mean breaking down the requirements into specific functions that need to be implemented.

Error detection and correction represent another crucial capability. Reasoning models don't just barrel forward with their first approach—they've learned to recognize when a line of reasoning isn't working and to try alternative approaches. This metacognitive ability emerges from training on examples where multiple solution attempts are shown, including failed attempts and the subsequent corrections. The model learns to recognize the signatures of faulty reasoning and to backtrack when necessary.

The interplay between the model's knowledge base and its reasoning capabilities is particularly interesting. Unlike symbolic AI systems that separate knowledge from reasoning mechanisms, reasoning models integrate both in their neural networks. The same parameters that encode facts about the world also encode strategies for reasoning about those facts. This integration allows for flexible reasoning that can adapt to novel situations by drawing analogies to previously seen problems.

One of the most sophisticated aspects of reasoning models is their handling of uncertainty. Traditional LLMs often exhibit false confidence, stating incorrect information as if it were certain. Reasoning models, through their training process, learn to express uncertainty appropriately. They might say "I'm not certain, but here's my reasoning..." or "This approach might work, although there's another possibility..." This calibrated uncertainty emerges from training on examples that include explicit uncertainty markers and from the model's ability to recognize when multiple valid approaches exist.

The computational flow within a reasoning model during problem-solving is intricate. When presented with a problem, the model first engages in what we might call "problem understanding"—parsing the question, identifying key constraints, and recognizing what type of problem it's dealing with. This initial phase sets the stage for the reasoning process that follows.

Next comes the planning phase, where the model determines its approach. This isn't always explicit in the output, but analysis of the model's internal states shows activation patterns consistent with high-level planning. The model is essentially deciding whether to use a direct approach, whether decomposition is needed, what tools or techniques might be relevant, and what the overall solution strategy should be.

The execution phase involves working through the plan step by step. Here, the model's ability to maintain focus while juggling multiple pieces of information becomes crucial. It must keep track of where it is in the solution process, what it has established so far, and what remains to be done. This is where the enhanced working memory capabilities of reasoning models really shine.

Throughout the process, the model engages in continuous monitoring and adjustment. If an approach isn't working, it recognizes this and adjusts. If it realizes it made an error earlier, it can backtrack and correct it. This flexibility is what allows reasoning models to tackle problems that would stump traditional LLMs.

The output generation phase in reasoning models is also unique. The model must decide how much of its internal reasoning to expose, how to format the solution for clarity, and how to ensure the final answer addresses the original question. This involves a kind of compression and summarization of the extensive internal reasoning process.

Understanding how reasoning models work also requires appreciating what they're not doing. They're not executing formal logic in the way a theorem prover would. They're not maintaining explicit symbolic representations like a traditional AI system. Instead, they're performing a kind of "soft reasoning" that's learned from examples and embedded in neural networks. This approach has both advantages—like flexibility and the ability to handle informal reasoning—and disadvantages, which we'll explore in later chapters.

The remarkable thing about reasoning models is that all of this complex behavior emerges from the relatively simple foundation of predicting the next token in a sequence. Through careful training and architectural innovations, these models have learned to use this basic capability to build complex chains of reasoning that can solve problems requiring genuine intellectual work. It's a testament to the power of the transformer architecture and the insights of researchers who recognized that reasoning could be cast as a sequence modeling problem.

As we'll see in the following chapters, the details of how these models are trained and how they apply their reasoning capabilities to real-world problems reveal even more fascinating aspects of this technology. But at its core, the functioning of reasoning models represents a beautiful synthesis of pattern recognition, sequential processing, and learned problem-solving strategies—all implemented in the distributed representations of neural networks.
